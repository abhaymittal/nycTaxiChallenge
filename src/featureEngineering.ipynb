{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "try:\n",
    "    import cPickle as p\n",
    "except:\n",
    "    import Pickle as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainData=pd.read_csv('../data/train.csv')\n",
    "testData=pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filterData(dataFrame):\n",
    "    long_limit=[-74.257159, -73.699215]\n",
    "    lat_limit=[40.495992, 40.915568]\n",
    "    dataFrame=dataFrame[(dataFrame['pickup_longitude']>=long_limit[0])&(dataFrame['pickup_longitude']<=long_limit[1])]\n",
    "    dataFrame=dataFrame[(dataFrame['pickup_latitude']>=lat_limit[0])&(dataFrame['pickup_latitude']<=lat_limit[1])]\n",
    "    dataFrame=dataFrame[(dataFrame['dropoff_longitude']>=long_limit[0])&(dataFrame['dropoff_longitude']<=long_limit[1])]\n",
    "    dataFrame=dataFrame[(dataFrame['dropoff_latitude']>=lat_limit[0])&(dataFrame['dropoff_latitude']<=lat_limit[1])]\n",
    "    return dataFrame\n",
    "\n",
    "trainData=trainData[trainData['trip_duration']>=60]\n",
    "trainData=trainData[trainData['trip_duration']<1939736]\n",
    "\n",
    "trainData=filterData(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id' 'vendor_id' 'pickup_datetime' 'dropoff_datetime' 'passenger_count'\n",
      " 'pickup_longitude' 'pickup_latitude' 'dropoff_longitude'\n",
      " 'dropoff_latitude' 'store_and_fwd_flag' 'trip_duration']\n"
     ]
    }
   ],
   "source": [
    "print trainData.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin by copying the columns which are not going to be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=pd.DataFrame(trainData[['id','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count','vendor_id']])\n",
    "yTrain=pd.Series(trainData['trip_duration'])\n",
    "test=pd.DataFrame(testData[['id','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count','vendor_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id' 'pickup_longitude' 'pickup_latitude' 'dropoff_longitude'\n",
      " 'dropoff_latitude' 'passenger_count' 'vendor_id' 'store_and_fwd_flag']\n"
     ]
    }
   ],
   "source": [
    "train['store_and_fwd_flag']=[1 if x=='Y' else 0 for x in trainData['store_and_fwd_flag']]\n",
    "test['store_and_fwd_flag']=[1 if x=='Y' else 0 for x in testData['store_and_fwd_flag']]\n",
    "print train.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets deal with latitude and longitude features. I was confused on what is the proper way to encode these features and saw that [beluga](https://www.kaggle.com/gaborfodor/from-eda-to-the-top-lb-0-367) used PCA which kind of makes sense as PCA turns a set of correlated features into uncorrelated ones. I'll use it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coords = np.vstack((train[['pickup_longitude', 'pickup_latitude']].values,\n",
    "                    train[['dropoff_longitude', 'dropoff_latitude']].values,\n",
    "                    test[['pickup_longitude', 'pickup_latitude']].values,\n",
    "                    test[['dropoff_longitude', 'dropoff_latitude']].values))\n",
    "pca = PCA().fit(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_pickup_pca=pca.transform(train[['pickup_longitude', 'pickup_latitude']])\n",
    "train['pickup_pca0']=train_pickup_pca[:,0]\n",
    "train['pickup_pca1']=train_pickup_pca[:,1]\n",
    "\n",
    "train_dropoff_pca=pca.transform(train[['dropoff_longitude','dropoff_latitude']])\n",
    "train['dropoff_pca0']=train_dropoff_pca[:,0]\n",
    "train['dropoff_pca1']=train_dropoff_pca[:,1]\n",
    "\n",
    "test_pickup_pca=pca.transform(test[['pickup_longitude', 'pickup_latitude']])\n",
    "test['pickup_pca0']=test_pickup_pca[:,0]\n",
    "test['pickup_pca1']=test_pickup_pca[:,1]\n",
    "\n",
    "test_dropoff_pca=pca.transform(test[['dropoff_longitude','dropoff_latitude']])\n",
    "test['dropoff_pca0']=test_dropoff_pca[:,0]\n",
    "test['dropoff_pca1']=test_dropoff_pca[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets add distance between pickup and dropoff points. Note that I'll also add bearing as I mentioned in EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "    (https://stackoverflow.com/questions/29545704/fast-haversine-approximation-python-pandas)\n",
    "    All args must be of equal length.    \n",
    "\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6367 * c\n",
    "    return km\n",
    "\n",
    "train['dist_haversine']=haversine_np(train['pickup_longitude'],train['pickup_latitude'],train['dropoff_longitude'],train['dropoff_latitude'])\n",
    "test['dist_haversine']=haversine_np(test['pickup_longitude'],test['pickup_latitude'],test['dropoff_longitude'],test['dropoff_latitude'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also add manhattan distances to it (because we've already assumed the surface to be flat enough to be considered euclidean and used kmeans on it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manhattan_dist(x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    Calculate manhattan distance between two points\n",
    "    \"\"\"\n",
    "    return abs(x1-x2)+abs(y1-y2)\n",
    "\n",
    "train['dist_pca_manhattan']=manhattan_dist(train['pickup_pca0'],train['pickup_pca1'],train['dropoff_pca0'],train['dropoff_pca1'])\n",
    "test['dist_pca_manhattan']=manhattan_dist(test['pickup_pca0'],test['pickup_pca1'],test['dropoff_pca0'],test['dropoff_pca1'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's add bearing now (Formula from http://www.movable-type.co.uk/scripts/latlong.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bearing(lon1,lat1,lon2,lat2):\n",
    "    \"\"\"\n",
    "    Calculate bearing angle\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    y=np.sin(dlon)*np.cos(lat2)\n",
    "    x=np.cos(lat1)*np.sin(lat2)-np.sin(lat1)*np.cos(lat2)*np.cos(dlon)\n",
    "    return np.degrees(np.arctan2(y,x))\n",
    "\n",
    "train['bearing']=bearing(train['pickup_longitude'],train['pickup_latitude'],train['dropoff_longitude'],train['dropoff_latitude'])\n",
    "test['bearing']=bearing(test['pickup_longitude'],test['pickup_latitude'],test['dropoff_longitude'],test['dropoff_latitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the latitude and distance calculations done, lets encode pickup and dropoff clusters too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans=p.load(open('../pickle_dumps/kmeansDump.p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_clusters=kmeans.predict(train[['pickup_longitude','pickup_latitude']])\n",
    "enc=OneHotEncoder()\n",
    "y=enc.fit_transform(train_clusters.reshape(-1,1))\n",
    "y=y.toarray()\n",
    "train['pickup_cluster_0']=y[:,0]\n",
    "train['pickup_cluster_1']=y[:,1]\n",
    "train['pickup_cluster_2']=y[:,2]\n",
    "train['pickup_cluster_3']=y[:,3]\n",
    "train['pickup_cluster_4']=y[:,4]\n",
    "train['pickup_cluster_5']=y[:,5]\n",
    "train['pickup_cluster_6']=y[:,6]\n",
    "train['pickup_cluster_7']=y[:,7]\n",
    "train['pickup_cluster_8']=y[:,8]\n",
    "train['pickup_cluster_9']=y[:,9]\n",
    "train['pickup_cluster_10']=y[:,10]\n",
    "train['pickup_cluster_11']=y[:,11]\n",
    "train['pickup_cluster_12']=y[:,12]\n",
    "train['pickup_cluster_13']=y[:,13]\n",
    "train['pickup_cluster_14']=y[:,14]\n",
    "train['pickup_cluster_15']=y[:,15]\n",
    "del y\n",
    "del train_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_clusters=kmeans.predict(train[['dropoff_longitude','dropoff_latitude']])\n",
    "enc=OneHotEncoder()\n",
    "y=enc.fit_transform(train_clusters.reshape(-1,1))\n",
    "y=y.toarray()\n",
    "train['dropoff_cluster_0']=y[:,0]\n",
    "train['dropoff_cluster_1']=y[:,1]\n",
    "train['dropoff_cluster_2']=y[:,2]\n",
    "train['dropoff_cluster_3']=y[:,3]\n",
    "train['dropoff_cluster_4']=y[:,4]\n",
    "train['dropoff_cluster_5']=y[:,5]\n",
    "train['dropoff_cluster_6']=y[:,6]\n",
    "train['dropoff_cluster_7']=y[:,7]\n",
    "train['dropoff_cluster_8']=y[:,8]\n",
    "train['dropoff_cluster_9']=y[:,9]\n",
    "train['dropoff_cluster_10']=y[:,10]\n",
    "train['dropoff_cluster_11']=y[:,11]\n",
    "train['dropoff_cluster_12']=y[:,12]\n",
    "train['dropoff_cluster_13']=y[:,13]\n",
    "train['dropoff_cluster_14']=y[:,14]\n",
    "train['dropoff_cluster_15']=y[:,15]\n",
    "del y\n",
    "del train_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_clusters=kmeans.predict(test[['pickup_longitude','pickup_latitude']])\n",
    "enc=OneHotEncoder()\n",
    "y=enc.fit_transform(test_clusters.reshape(-1,1))\n",
    "y=y.toarray()\n",
    "test['pickup_cluster_0']=y[:,0]\n",
    "test['pickup_cluster_1']=y[:,1]\n",
    "test['pickup_cluster_2']=y[:,2]\n",
    "test['pickup_cluster_3']=y[:,3]\n",
    "test['pickup_cluster_4']=y[:,4]\n",
    "test['pickup_cluster_5']=y[:,5]\n",
    "test['pickup_cluster_6']=y[:,6]\n",
    "test['pickup_cluster_7']=y[:,7]\n",
    "test['pickup_cluster_8']=y[:,8]\n",
    "test['pickup_cluster_9']=y[:,9]\n",
    "test['pickup_cluster_10']=y[:,10]\n",
    "test['pickup_cluster_11']=y[:,11]\n",
    "test['pickup_cluster_12']=y[:,12]\n",
    "test['pickup_cluster_13']=y[:,13]\n",
    "test['pickup_cluster_14']=y[:,14]\n",
    "test['pickup_cluster_15']=y[:,15]\n",
    "del y\n",
    "del test_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_clusters=kmeans.predict(test[['dropoff_longitude','dropoff_latitude']])\n",
    "enc=OneHotEncoder()\n",
    "y=enc.fit_transform(test_clusters.reshape(-1,1))\n",
    "y=y.toarray()\n",
    "test['dropoff_cluster_0']=y[:,0]\n",
    "test['dropoff_cluster_1']=y[:,1]\n",
    "test['dropoff_cluster_2']=y[:,2]\n",
    "test['dropoff_cluster_3']=y[:,3]\n",
    "test['dropoff_cluster_4']=y[:,4]\n",
    "test['dropoff_cluster_5']=y[:,5]\n",
    "test['dropoff_cluster_6']=y[:,6]\n",
    "test['dropoff_cluster_7']=y[:,7]\n",
    "test['dropoff_cluster_8']=y[:,8]\n",
    "test['dropoff_cluster_9']=y[:,9]\n",
    "test['dropoff_cluster_10']=y[:,10]\n",
    "test['dropoff_cluster_11']=y[:,11]\n",
    "test['dropoff_cluster_12']=y[:,12]\n",
    "test['dropoff_cluster_13']=y[:,13]\n",
    "test['dropoff_cluster_14']=y[:,14]\n",
    "test['dropoff_cluster_15']=y[:,15]\n",
    "del y\n",
    "del test_clusters\n",
    "del kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets handle the date time features. We'll handle cyclic features by taking their sine and cos values as mentioned at https://datascience.stackexchange.com/questions/5990/what-is-a-good-way-to-transform-cyclic-ordinal-attributes. I will also keep their linearity in mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainDT=pd.to_datetime(trainData.pickup_datetime)\n",
    "testDT=pd.to_datetime(testData.pickup_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['pickup_hour']=trainDT.dt.hour\n",
    "train['pickup_hour_sin']=np.sin((np.pi*2*train['pickup_hour']*1.0)/24)\n",
    "train['pickup_hour_cos']=np.cos((np.pi*2*train['pickup_hour']*1.0)/24)\n",
    "\n",
    "test['pickup_hour']=testDT.dt.hour\n",
    "test['pickup_hour_sin']=np.sin((np.pi*2*test['pickup_hour']*1.0)/24)\n",
    "test['pickup_hour_cos']=np.cos((np.pi*2*test['pickup_hour']*1.0)/24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['pickup_dow']=trainDT.dt.dayofweek\n",
    "train['pickup_dow_sin']=np.sin((np.pi*2*train['pickup_dow']*1.0)/7)\n",
    "train['pickup_dow_cos']=np.cos((np.pi*2*train['pickup_dow']*1.0)/7)\n",
    "\n",
    "test['pickup_dow']=testDT.dt.dayofweek\n",
    "test['pickup_dow_sin']=np.sin((np.pi*2*test['pickup_dow']*1.0)/7)\n",
    "test['pickup_dow_cos']=np.cos((np.pi*2*test['pickup_dow']*1.0)/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['pickup_month']=trainDT.dt.month\n",
    "train['pickup_month_sin']=np.sin((np.pi*2*train['pickup_month']*1.0)/12)\n",
    "train['pickup_month_cos']=np.cos((np.pi*2*train['pickup_month']*1.0)/12)\n",
    "\n",
    "test['pickup_month']=testDT.dt.month\n",
    "test['pickup_month_sin']=np.sin((np.pi*2*test['pickup_month']*1.0)/12)\n",
    "test['pickup_month_cos']=np.cos((np.pi*2*test['pickup_month']*1.0)/12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['pickup_week_year']=trainDT.dt.weekofyear\n",
    "train['pickup_week_year_sin']=np.sin((np.pi*2*train['pickup_week_year']*1.0)/53)\n",
    "train['pickup_week_year_cos']=np.cos((np.pi*2*train['pickup_week_year']*1.0)/53)\n",
    "\n",
    "test['pickup_week_year']=testDT.dt.weekofyear\n",
    "test['pickup_week_year_sin']=np.sin((np.pi*2*test['pickup_week_year']*1.0)/53)\n",
    "test['pickup_week_year_cos']=np.cos((np.pi*2*test['pickup_week_year']*1.0)/53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=trainDT.apply(lambda x:1 if x.dayofweek>=4 and (x.hour<=6 or x.hour>=21) else 0)\n",
    "train['party_night_traffic']=x\n",
    "x=testDT.apply(lambda x:1 if x.dayofweek>=4 and (x.hour<=6 or x.hour>=21) else 0)\n",
    "test['party_night_traffic']=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=trainDT.apply(lambda x:1 if x.dayofweek<=4 and (x.hour>=8 or x.hour<=19) else 0)\n",
    "train['work_day_traffic']=x\n",
    "x=testDT.apply(lambda x:1 if x.dayofweek<=4 and (x.hour>=8 or x.hour<=19) else 0)\n",
    "test['work_day_traffic']=x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us add features from other datasets. Lets add routing data from OSRM provided by oscarleo (https://www.kaggle.com/oscarleo/new-york-city-taxi-with-osrm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_osrm_1=pd.read_csv('../data/fastest_routes_train_part_1.csv', usecols=['id', 'total_distance', 'total_travel_time',  'number_of_steps'])\n",
    "tr_osrm_2=pd.read_csv('../data/fastest_routes_train_part_2.csv', usecols=['id', 'total_distance', 'total_travel_time',  'number_of_steps'])\n",
    "tr_osrm=pd.concat((tr_osrm_1, tr_osrm_2))\n",
    "te_osrm=pd.read_csv('../data/fastest_routes_test.csv',\n",
    "                               usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\n",
    "\n",
    "train=train.merge(tr_osrm,how='left',on='id')\n",
    "test=test.merge(te_osrm,how='left',on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del train['id']\n",
    "del test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pickup_longitude' 'pickup_latitude' 'dropoff_longitude'\n",
      " 'dropoff_latitude' 'passenger_count' 'vendor_id' 'store_and_fwd_flag'\n",
      " 'pickup_pca0' 'pickup_pca1' 'dropoff_pca0' 'dropoff_pca1' 'dist_haversine'\n",
      " 'dist_pca_manhattan' 'bearing' 'pickup_cluster_0' 'pickup_cluster_1'\n",
      " 'pickup_cluster_2' 'pickup_cluster_3' 'pickup_cluster_4'\n",
      " 'pickup_cluster_5' 'pickup_cluster_6' 'pickup_cluster_7'\n",
      " 'pickup_cluster_8' 'pickup_cluster_9' 'pickup_cluster_10'\n",
      " 'pickup_cluster_11' 'pickup_cluster_12' 'pickup_cluster_13'\n",
      " 'pickup_cluster_14' 'pickup_cluster_15' 'dropoff_cluster_0'\n",
      " 'dropoff_cluster_1' 'dropoff_cluster_2' 'dropoff_cluster_3'\n",
      " 'dropoff_cluster_4' 'dropoff_cluster_5' 'dropoff_cluster_6'\n",
      " 'dropoff_cluster_7' 'dropoff_cluster_8' 'dropoff_cluster_9'\n",
      " 'dropoff_cluster_10' 'dropoff_cluster_11' 'dropoff_cluster_12'\n",
      " 'dropoff_cluster_13' 'dropoff_cluster_14' 'dropoff_cluster_15'\n",
      " 'pickup_hour' 'pickup_hour_sin' 'pickup_hour_cos' 'pickup_dow'\n",
      " 'pickup_dow_sin' 'pickup_dow_cos' 'pickup_month' 'pickup_month_sin'\n",
      " 'pickup_month_cos' 'pickup_week_year' 'pickup_week_year_sin'\n",
      " 'pickup_week_year_cos' 'party_night_traffic' 'work_day_traffic'\n",
      " 'total_distance' 'total_travel_time' 'number_of_steps']\n",
      "63\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "print train.columns.values\n",
    "print len(train.columns.values)\n",
    "print len(test.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.to_csv('../data/train_final.csv',index=False)\n",
    "test.to_csv('../data/test_final.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
